#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IT ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
ì—¬ëŸ¬ IT ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import json
import feedparser
from datetime import datetime
import re

# IT ë‰´ìŠ¤ RSS í”¼ë“œ ëª©ë¡
NEWS_SOURCES = {
    'techcrunch': 'https://techcrunch.com/feed/',
    'theverge': 'https://www.theverge.com/rss/index.xml',
    'hackernews': 'https://hnrss.org/frontpage',
    'arstechnica': 'https://feeds.arstechnica.com/arstechnica/index',
    'wired': 'https://www.wired.com/feed/rss',
}

def clean_html(text):
    """HTML íƒœê·¸ ì œê±°"""
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

def extract_keywords(title, summary):
    """ì œëª©ê³¼ ìš”ì•½ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    keywords = []
    
    # IT ê´€ë ¨ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    tech_keywords = [
        'AI', 'ChatGPT', 'GPT', 'OpenAI', 'Anthropic', 'Claude',
        'Python', 'JavaScript', 'TypeScript', 'Rust', 'Go', 'Java',
        'React', 'Vue', 'Angular', 'Next.js', 'Node.js',
        'Docker', 'Kubernetes', 'AWS', 'Azure', 'Google Cloud',
        'Machine Learning', 'Deep Learning', 'Neural Network',
        'Blockchain', 'Web3', 'Crypto', 'NFT',
        'API', 'REST', 'GraphQL', 'Microservices',
        'Security', 'Cybersecurity', 'Hack', 'Vulnerability',
        'Startup', 'Funding', 'VC', 'IPO',
        'Apple', 'Google', 'Microsoft', 'Meta', 'Amazon', 'Tesla',
        'iPhone', 'Android', 'iOS', 'Windows', 'Linux',
        'Database', 'SQL', 'NoSQL', 'MongoDB', 'PostgreSQL',
        'DevOps', 'CI/CD', 'Git', 'GitHub',
        '5G', 'IoT', 'Cloud', 'Edge Computing',
        'VR', 'AR', 'Metaverse', 'Gaming',
    ]
    
    text = f"{title} {summary}".lower()
    
    for keyword in tech_keywords:
        if keyword.lower() in text:
            keywords.append(keyword)
    
    return keywords[:5]  # ìµœëŒ€ 5ê°œ

def categorize_news(title, summary, keywords):
    """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    text = f"{title} {summary}".lower()
    
    categories = {
        'AI/ML': ['ai', 'artificial intelligence', 'machine learning', 'gpt', 'chatgpt', 'claude', 'llm', 'neural'],
        'Programming': ['python', 'javascript', 'rust', 'programming', 'code', 'developer', 'typescript'],
        'Cloud/DevOps': ['cloud', 'aws', 'azure', 'docker', 'kubernetes', 'devops', 'ci/cd'],
        'Security': ['security', 'hack', 'breach', 'vulnerability', 'cyber'],
        'Blockchain': ['blockchain', 'crypto', 'bitcoin', 'ethereum', 'web3', 'nft'],
        'Mobile': ['iphone', 'android', 'ios', 'mobile', 'app'],
        'Startup': ['startup', 'funding', 'vc', 'investment', 'ipo'],
        'Hardware': ['chip', 'processor', 'gpu', 'hardware', 'device'],
    }
    
    for category, keywords_list in categories.items():
        for keyword in keywords_list:
            if keyword in text:
                return category
    
    return 'Tech News'

def crawl_news():
    """ë‰´ìŠ¤ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
    all_news = []
    
    print("ğŸ” IT ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
    
    for source_name, feed_url in NEWS_SOURCES.items():
        try:
            print(f"ğŸ“° {source_name} í¬ë¡¤ë§ ì¤‘...")
            feed = feedparser.parse(feed_url)
            
            for entry in feed.entries[:5]:  # ê° ì†ŒìŠ¤ì—ì„œ ìµœì‹  5ê°œ
                title = entry.get('title', '')
                summary = clean_html(entry.get('summary', entry.get('description', '')))
                link = entry.get('link', '')
                
                # ë°œí–‰ì¼ íŒŒì‹±
                published = entry.get('published', entry.get('updated', ''))
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = extract_keywords(title, summary)
                
                # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                category = categorize_news(title, summary, keywords)
                
                news_item = {
                    'source': source_name,
                    'title': title,
                    'summary': summary[:300],  # ìš”ì•½ì€ 300ìë¡œ ì œí•œ
                    'link': link,
                    'published': published,
                    'keywords': keywords,
                    'category': category,
                    'crawled_at': datetime.now().isoformat()
                }
                
                all_news.append(news_item)
                
        except Exception as e:
            print(f"âŒ {source_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            continue
    
    print(f"âœ… ì´ {len(all_news)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    return all_news

def save_news(news_list, filename='news-data.json'):
    """ë‰´ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    data = {
        'last_updated': datetime.now().isoformat(),
        'news_count': len(news_list),
        'news': news_list
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ {filename}ì— ì €ì¥ ì™„ë£Œ!")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 50)
    print("IT ë‰´ìŠ¤ í¬ë¡¤ëŸ¬")
    print("=" * 50)
    
    # ë‰´ìŠ¤ í¬ë¡¤ë§
    news = crawl_news()
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    save_news(news)
    
    print("\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"ğŸ“… ì—…ë°ì´íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()
